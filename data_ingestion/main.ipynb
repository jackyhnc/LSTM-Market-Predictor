{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb25c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta, datetime\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "from gdeltdoc.errors import RateLimitError\n",
    "from urllib.parse import quote\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "START_DATE = date(year=2018, month=1, day=1)\n",
    "END_DATE = datetime.now().date()\n",
    "DATE_INCREMENT = timedelta(days=31)\n",
    "\n",
    "gdd = GdeltDoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32bc331",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = pd.read_csv(\"../sp500.csv\")\n",
    "tickers = sorted([ticker for ticker in sp500[\"Symbol\"]])\n",
    "symbol_to_security = sp500.set_index(\"Symbol\")[\"Security\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2b394a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c5/nwrhnbhs5ks1ssf190f4b49r0000gn/T/ipykernel_51590/2168569912.py:1: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  sp500_data = yf.download(tickers, start=\"2018-01-01\", end=\"2025-09-01\", group_by=\"ticker\")\n",
      "[*********             19%                       ]  98 of 503 completedHTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: ANSS\"}}}\n",
      "[*************         27%                       ]  136 of 503 completedHTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: HES\"}}}\n",
      "[*********************100%***********************]  503 of 503 completed\n",
      "\n",
      "6 Failed downloads:\n",
      "['BF.B']: YFPricesMissingError('possibly delisted; no price data found  (1d 2018-01-01 -> 2025-09-01)')\n",
      "['ANSS', 'HES', 'BRK.B']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "['PARA']: YFPricesMissingError('possibly delisted; no price data found  (1d 2018-01-01 -> 2025-09-01) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "['O']: Timeout('Failed to perform, curl: (28) Connection timed out after 10002 milliseconds. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n"
     ]
    }
   ],
   "source": [
    "sp500_data = yf.download(tickers, start=\"2018-01-01\", end=\"2025-09-01\", group_by=\"ticker\")\n",
    "sp500_data.to_csv(\"sp500_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ecd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_PATH = \"cached_articles\"\n",
    "IGNORE_PATH = os.path.join(CACHE_PATH, \"ignored.json\")\n",
    "\n",
    "\n",
    "def is_ignored_article(ticker: str, start_date: str):\n",
    "    if os.path.exists(IGNORE_PATH):\n",
    "        with open(IGNORE_PATH, \"r\") as file:\n",
    "            data = json.loads(file.read())\n",
    "            return start_date in data and ticker in data[start_date]\n",
    "    return False\n",
    "\n",
    "\n",
    "def set_ignored_article(ticker: str, start_date: str):\n",
    "    os.makedirs(CACHE_PATH, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(IGNORE_PATH):\n",
    "        with open(IGNORE_PATH, \"r\") as file:\n",
    "            data = json.loads(file.read())\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    if start_date not in data:\n",
    "        data[start_date] = []\n",
    "\n",
    "    if ticker not in data[start_date]:\n",
    "        data[start_date].append(ticker)\n",
    "\n",
    "    with open(IGNORE_PATH, \"w\") as file:\n",
    "        file.write(json.dumps(data, indent=4))\n",
    "\n",
    "\n",
    "def save_articles_to_file(ticker: str, start_date: str, end_date: str, *, save_file: str, index: int, length: int):\n",
    "    filters = Filters(keyword=quote(symbol_to_security[ticker]), start_date=start_date, end_date=end_date)\n",
    "    articles = gdd.article_search(filters)\n",
    "\n",
    "    if len(articles) > 0:\n",
    "        articles = articles[articles[\"language\"] == \"English\"]\n",
    "        articles = articles[articles[\"sourcecountry\"] == \"United States\"]\n",
    "\n",
    "        os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "        articles.to_csv(save_file)\n",
    "        print(\n",
    "            f\"({index:3d}/{length}) {'\\033[38;5;6m'}DOWNLOADED DATA{'\\033[0m'}: {'\\033[38;5;7m'}{start_date} {'\\033[38;5;3m'}{ticker} ({symbol_to_security[ticker]}){'\\033[0m'}\"\n",
    "        )\n",
    "    else:\n",
    "        set_ignored_article(ticker, start_date)\n",
    "        print(\n",
    "            f\"({index:3d}/{length}) {'\\033[38;5;1m'}NO DATA{'\\033[0m'}: {'\\033[38;5;7m'}{start_date} {'\\033[38;5;3m'}{ticker} ({symbol_to_security[ticker]}){'\\033[0m'}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def save_articles(ticker: str, *, index: int, length: int):\n",
    "    if len(symbol_to_security[ticker]) < 5:\n",
    "        print(f\"({index:3d}/{length}) {'\\033[38;5;1m'}BAD DATA{'\\033[0m'}: {'\\033[38;5;3m'}{ticker} ({symbol_to_security[ticker]}){'\\033[0m'}\")\n",
    "        return\n",
    "\n",
    "    start_date = START_DATE\n",
    "    while start_date + DATE_INCREMENT < END_DATE:\n",
    "        save_file = f\"{CACHE_PATH}/{start_date}/{ticker}.csv\"\n",
    "\n",
    "        if is_ignored_article(ticker, str(start_date)):\n",
    "            print(\n",
    "                f\"({index:3d}/{length}) {'\\033[38;5;1m'}NO DATA{'\\033[0m'}: {'\\033[38;5;7m'}{start_date} {'\\033[38;5;3m'}{ticker} ({symbol_to_security[ticker]}){'\\033[0m'}\"\n",
    "            )\n",
    "            # pass\n",
    "        elif os.path.exists(save_file):\n",
    "            print(f\"({index:3d}/{length}) {'\\033[38;5;5m'}SKIPPED DATA{'\\033[0m'}: {'\\033[38;5;7m'}{start_date} {'\\033[38;5;3m'}{ticker}{'\\033[0m'}\")\n",
    "            # pass\n",
    "        else:\n",
    "            save_articles_to_file(ticker, str(start_date), str(start_date + DATE_INCREMENT), save_file=save_file, index=index, length=length)\n",
    "\n",
    "        start_date += DATE_INCREMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tickers = tickers[0:101]\n",
    "\n",
    "for i, ticker in enumerate(selected_tickers):\n",
    "    while True:\n",
    "        start = time.time()\n",
    "\n",
    "        try:\n",
    "            save_articles(ticker, index=i, length=len(selected_tickers))\n",
    "            print(f\"TOTAL TIME: {time.time() - start:.2f} (s)\")\n",
    "            break\n",
    "        except RateLimitError:\n",
    "            print(f\"{'\\033[38;5;1m'}RATE LIMIT REACHED. WAITING...{'\\033[0m'}\")\n",
    "            time.sleep(90)\n",
    "        except Exception as err:\n",
    "            print(f\"{'\\033[38;5;1m'}FAILED TO GET ALL DATA{'\\033[0m'}\")\n",
    "            raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05680d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tickers[0:101])  # indices 0–100 (DYLAN)\n",
    "print(tickers[101:202])  # indices 101–201 (EFORD)\n",
    "print(tickers[202:303])  # indices 202–302 (JACKY)\n",
    "print(tickers[303:403])  # indices 303–402 (CALVIN)\n",
    "print(tickers[403:503])  # indices 403–502 (TEJU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "os.makedirs(\"merged_articles\", exist_ok=True)\n",
    "\n",
    "for ticker in tickers:\n",
    "    files = glob.glob(\"E:/cached_articles/**/AAPL.csv\")\n",
    "    dfs = [pd.read_csv(file, index_col=0) for file in files]\n",
    "\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    merged_df.to_csv(f\"merged_articles/{ticker}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
